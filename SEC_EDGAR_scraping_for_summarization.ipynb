{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b383d1",
   "metadata": {},
   "source": [
    "# Retrieval of All Content from SEC Filings to Support RAG and Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d1789",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8daf6eb",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07086d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Literal\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eed5ec",
   "metadata": {},
   "source": [
    "Define the user agent details for the SEC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0dcb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENT = \"AirlineFinancialDashboard.com (michael.tricanowicz@live.com)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca91d52",
   "metadata": {},
   "source": [
    "Define functions to create period start and end date windows to filter filing retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec56fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create start and end dates\n",
    "def define_period_dates(year: int, period: str) -> Tuple[datetime, datetime]:\n",
    "    \"\"\"\n",
    "    period: 'FY' or 'Q1'/'Q2'/'Q3'/'Q4'\n",
    "    Returns: (start_date, end_date) as datetimes covering that period.\n",
    "    \"\"\"\n",
    "    if period == \"FY\":\n",
    "        start_month = 1\n",
    "        end_month = 12\n",
    "    else:\n",
    "        end_month = int(period[-1]) * 3\n",
    "        start_month = end_month - 2\n",
    "    start_day = 1\n",
    "    end_day = calendar.monthrange(year, end_month)[1]\n",
    "    # Create start and end date variables to constrain document scraping\n",
    "    start_date = datetime(year, start_month, start_day)\n",
    "    end_date = datetime(year, end_month, end_day)\n",
    "    return start_date, end_date\n",
    "\n",
    "# Define function to build date windows with labels when multiple years and/or periods are needed\n",
    "def build_date_windows(years: Iterable[int], periods: Iterable[str]) -> List[Tuple[str, datetime, datetime]]:\n",
    "    \"\"\"\n",
    "    Create a list of windows with labels, e.g.:\n",
    "    [('2024-Q1', start, end), ('2024-FY', start, end), ...]\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    for y in years:\n",
    "        for p in periods:\n",
    "            s, e = define_period_dates(y, p)\n",
    "            windows.append((f\"{y}-{p}\", s, e))\n",
    "    # Sort for stability / readability\n",
    "    windows.sort(key=lambda x: (x[1], x[0]))\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bef1a3",
   "metadata": {},
   "source": [
    "Define functions to create maps of airlines of interest and their CIKs for filing retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a5c0197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to fetch ticker to CIK mapping\n",
    "def fetch_ticker_to_cik_map(user_agent: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Download SEC's company_tickers.json ONCE and build {ticker -> 10-digit CIK}.\n",
    "\n",
    "    Why:\n",
    "    - Avoids repeated downloads per ticker\n",
    "    - Faster, more polite, fewer rate-limit issues\n",
    "    \"\"\"\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    resp = requests.get(url, headers=headers, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    ticker_to_cik = {}\n",
    "    for item in data.values():\n",
    "        ticker = item[\"ticker\"].upper()\n",
    "        cik10 = str(item[\"cik_str\"]).zfill(10)\n",
    "        ticker_to_cik[ticker] = cik10\n",
    "    return ticker_to_cik\n",
    "\n",
    "# Define function to build airline CIK dictionary\n",
    "def build_airline_cik_dict(airlines: List[str], ticker_to_cik: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Convert a list of tickers into {ticker: cik10}. Skips unknown tickers.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for t in airlines:\n",
    "        t_up = t.upper()\n",
    "        cik = ticker_to_cik.get(t_up)\n",
    "        if cik:\n",
    "            out[t_up] = cik\n",
    "    return out\n",
    "\n",
    "# Fetch and define the ticker mapping dictionary\n",
    "ticker_map = fetch_ticker_to_cik_map(USER_AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16672ab",
   "metadata": {},
   "source": [
    "## Financial Performance Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb779d50",
   "metadata": {},
   "source": [
    "**Download Complete Filings (text + tables) via Submissions → Archives index.json → download docs**<br>\n",
    "\n",
    "For each company:\n",
    "- Use Submissions JSON to list filings and get the accession number + primary document name.\n",
    "- For each accession, go to the Archives folder and fetch index.json to enumerate all documents in the submission.\n",
    "\n",
    "Download:\n",
    "- primaryDocument (often .htm)\n",
    "- {accession}.txt (“complete submission text file”) when available\n",
    "- optionally: XBRL instance files (.xml, .xsd, .cal, .lab, .pre) and other HTML exhibits\n",
    "\n",
    "Parse the primary HTML into:\n",
    "- a cleaned narrative text file for embedding\n",
    "- a set of tables exported to CSV\n",
    "\n",
    "Why this approach:\n",
    "It’s the most reliable way to capture full narrative + tables (beyond just XBRL facts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556fb228",
   "metadata": {},
   "source": [
    "### **Helpers + SEC client Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f356829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def normalize_cik(cik: str | int) -> str:\n",
    "    \"\"\"\n",
    "    Convert CIK to a zero-padded 10-digit string.\n",
    "\n",
    "    Why:\n",
    "    - SEC endpoints typically want 'CIK##########' where the CIK is 10 digits.\n",
    "    - Users commonly provide CIKs with fewer digits or with formatting.\n",
    "    \"\"\"\n",
    "    s = re.sub(r\"[^\\d]\", \"\", str(cik))  # keep digits only\n",
    "    return s.zfill(10)\n",
    "\n",
    "def cik_archives_path(cik: str | int) -> str:\n",
    "    \"\"\"\n",
    "    Convert CIK to the Archives path form.\n",
    "\n",
    "    Why:\n",
    "    - Archives URLs use the integer form of the CIK (no leading zeros).\n",
    "      Example: CIK 0000320193 becomes '320193' in /edgar/data/320193/\n",
    "    \"\"\"\n",
    "    return str(int(normalize_cik(cik)))\n",
    "\n",
    "def accession_nodashes(accession: str) -> str:\n",
    "    \"\"\"\n",
    "    Archives folder uses accession with dashes removed.\n",
    "\n",
    "    Example:\n",
    "    - '0000320193-24-000123' -> '000032019324000123'\n",
    "    \"\"\"\n",
    "    return accession.replace(\"-\", \"\").strip()\n",
    "\n",
    "def ensure_dir(p: Path) -> None:\n",
    "    \"\"\"Create a directory (and parents) if it doesn't exist.\"\"\"\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def parse_yyyy_mm_dd(s: Optional[str]) -> Optional[datetime]:\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(s, \"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def match_windows(\n",
    "    dt: datetime,\n",
    "    windows: List[Tuple[str, datetime, datetime]],\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Return labels of all windows that contain dt.\n",
    "    (Usually 0 or 1, but could be >1 if you have overlapping windows.)\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for label, start, end in windows:\n",
    "        if start <= dt <= end:\n",
    "            labels.append(label)\n",
    "    return labels\n",
    "\n",
    "# -----------------------------\n",
    "# SEC client with:\n",
    "# - descriptive User-Agent\n",
    "# - polite throttling (avoid hammering SEC)\n",
    "# - retry with backoff on transient errors\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class SecClient:\n",
    "    \"\"\"\n",
    "    Minimal SEC HTTP client.\n",
    "\n",
    "    Why a custom client:\n",
    "    - The SEC expects a descriptive User-Agent.\n",
    "    - You want consistent throttling and retry logic across all requests.\n",
    "    - You want one Session (connection reuse) for speed and politeness.\n",
    "    \"\"\"\n",
    "    user_agent: str\n",
    "    min_interval_sec: float = 0.15  # ~6-7 requests/sec. Safe under typical guidance.\n",
    "    timeout: int = 30\n",
    "    max_retries: int = 5\n",
    "\n",
    "    # SEC endpoints we will use\n",
    "    base_submissions: str = \"https://data.sec.gov/submissions\"\n",
    "    base_xbrl: str = \"https://data.sec.gov/api/xbrl\"\n",
    "    base_archives: str = \"https://www.sec.gov/Archives\"\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        # Use a requests.Session for connection pooling\n",
    "        self.sess = requests.Session()\n",
    "\n",
    "        # User-Agent is REQUIRED by SEC fair-access policy; include contact.\n",
    "        self.sess.headers.update({\n",
    "            \"User-Agent\": self.user_agent,\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"Accept\": \"application/json,text/html,text/plain,*/*\",\n",
    "        })\n",
    "\n",
    "        # Track last request time to enforce min interval between requests\n",
    "        self._last_ts = 0.0\n",
    "\n",
    "    def _throttle(self) -> None:\n",
    "        \"\"\"\n",
    "        Enforce a minimum time gap between requests.\n",
    "\n",
    "        Why:\n",
    "        - Keeps you compliant with SEC fair access expectations.\n",
    "        - Reduces 429 \"Too Many Requests\" errors.\n",
    "        \"\"\"\n",
    "        dt = time.time() - self._last_ts\n",
    "        if dt < self.min_interval_sec:\n",
    "            time.sleep(self.min_interval_sec - dt)\n",
    "\n",
    "    def get(self, url: str, *, stream: bool = False) -> requests.Response:\n",
    "        \"\"\"\n",
    "        GET with throttling + retry + exponential backoff.\n",
    "\n",
    "        Retries for:\n",
    "        - 429 rate limit\n",
    "        - 5xx server errors\n",
    "        \"\"\"\n",
    "        for attempt in range(1, self.max_retries + 1):\n",
    "            self._throttle()\n",
    "            try:\n",
    "                r = self.sess.get(url, timeout=self.timeout, stream=stream)\n",
    "                self._last_ts = time.time()\n",
    "\n",
    "                # Retry on common transient failures\n",
    "                if r.status_code in (429, 500, 502, 503, 504):\n",
    "                    time.sleep(min(2 ** attempt, 30))\n",
    "                    continue\n",
    "\n",
    "                r.raise_for_status()\n",
    "                return r\n",
    "\n",
    "            except requests.RequestException:\n",
    "                if attempt == self.max_retries:\n",
    "                    raise\n",
    "                time.sleep(min(2 ** attempt, 30))\n",
    "\n",
    "        raise RuntimeError(\"Unreachable: exceeded retry loop\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Submissions endpoint utilities\n",
    "    # -----------------------------\n",
    "    def submissions(self, cik: str | int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Retrieve the company submissions JSON:\n",
    "        https://data.sec.gov/submissions/CIK##########.json\n",
    "\n",
    "        Why:\n",
    "        - This is the best \"filing catalog\" for a company:\n",
    "          lists accession numbers, forms, dates, and primary document names.\n",
    "        \"\"\"\n",
    "        cik10 = normalize_cik(cik)\n",
    "        return self.get(f\"{self.base_submissions}/CIK{cik10}.json\").json()\n",
    "\n",
    "    def list_recent_filings(\n",
    "        self,\n",
    "        cik: str | int,\n",
    "        forms: Iterable[str],\n",
    "        limit: int = 500,\n",
    "        *,\n",
    "        windows: List[Tuple[str, datetime, datetime]],\n",
    "        date_field: Literal[\"filingDate\", \"reportDate\"] = \"reportDate\",\n",
    "        fallback_to_filing_date: bool = True,\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Return a list of recent filings for specific form types filtered on a date range using either reportDate (default) or filingDate.\n",
    "\n",
    "        Why date_field matters:\n",
    "        - filingDate: when SEC received/published the filing\n",
    "        - reportDate: end of reporting period (often aligns to FY/Q more cleanly)\n",
    "\n",
    "        Why:\n",
    "        - Submissions JSON stores 'recent' filings in a columnar structure:\n",
    "          recent['form'][i], recent['accessionNumber'][i], etc.\n",
    "        - This function converts it to row dictionaries (one per filing).\n",
    "        \"\"\"\n",
    "        data = self.submissions(cik)\n",
    "        recent = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "        if not recent:\n",
    "            return []\n",
    "\n",
    "        keys = list(recent.keys())\n",
    "        n = len(recent.get(\"accessionNumber\", []))\n",
    "        rows = [{k: recent[k][i] for k in keys} for i in range(n)]\n",
    "\n",
    "        # Filter by form type\n",
    "        forms_set = set(forms)\n",
    "        rows = [r for r in rows if r.get(\"form\") in forms_set]\n",
    "        \n",
    "        # Filter by date windows\n",
    "        filtered: List[Dict[str, Any]] = []\n",
    "        for r in rows:\n",
    "            dt = parse_yyyy_mm_dd(r.get(date_field))\n",
    "            if dt is None and fallback_to_filing_date and date_field != \"filingDate\":\n",
    "                dt = parse_yyyy_mm_dd(r.get(\"filingDate\"))\n",
    "            if dt is None:\n",
    "                continue\n",
    "            labels = match_windows(dt, windows)\n",
    "            if labels:\n",
    "                rr = dict(r)\n",
    "                rr[\"matched_windows\"] = labels\n",
    "                rr[\"filter_date_used\"] = dt.strftime(\"%Y-%m-%d\")\n",
    "                filtered.append(rr)\n",
    "\n",
    "        # Sort newest to oldest by chosen date field (so limit grabs most recent within range)\n",
    "        filtered.sort(\n",
    "            key=lambda r: (parse_yyyy_mm_dd(r.get(date_field)) or datetime.min),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        return filtered[:limit]\n",
    "    # -----------------------------\n",
    "    # Archives utilities\n",
    "    # -----------------------------\n",
    "    def filing_folder(self, cik: str | int, accession: str) -> str:\n",
    "        \"\"\"\n",
    "        Base folder for an accession, e.g.:\n",
    "        https://www.sec.gov/Archives/edgar/data/{cik_int}/{accession_nodashes}\n",
    "\n",
    "        Why:\n",
    "        - Every document (HTML, text, XBRL files, exhibits) is stored here.\n",
    "        \"\"\"\n",
    "        return f\"{self.base_archives}/edgar/data/{cik_archives_path(cik)}/{accession_nodashes(accession)}\"\n",
    "\n",
    "    def filing_index_json(self, cik: str | int, accession: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch index.json listing all files in the accession folder.\n",
    "\n",
    "        Why:\n",
    "        - \"Complete filing\" usually means more than one file.\n",
    "        - index.json is the authoritative list of docs to download.\n",
    "        \"\"\"\n",
    "        return self.get(f\"{self.filing_folder(cik, accession)}/index.json\").json()\n",
    "\n",
    "    def download_file(self, url: str, out_path: Path) -> None:\n",
    "        \"\"\"\n",
    "        Stream-download a file to disk.\n",
    "\n",
    "        Why stream:\n",
    "        - Some filings/exhibits are large.\n",
    "        - Streaming avoids holding the entire file in memory.\n",
    "        \"\"\"\n",
    "        ensure_dir(out_path.parent)\n",
    "        r = self.get(url, stream=True)\n",
    "        with out_path.open(\"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 256):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    def download_accession_package(\n",
    "        self,\n",
    "        cik: str | int,\n",
    "        accession: str,\n",
    "        primary_document: Optional[str],\n",
    "        out_dir: Path,\n",
    "        *,\n",
    "        include_patterns: Tuple[str, ...] = (\".htm\", \".html\", \".txt\", \".xml\", \".xsd\", \".cal\", \".lab\", \".pre\", \".json\"),\n",
    "        exclude_patterns: Tuple[str, ...] = (\".jpg\", \".png\", \".gif\", \".svg\", \".pdf\"),\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Download a \"complete package\" for an accession.\n",
    "\n",
    "        Downloads:\n",
    "        - index.json\n",
    "        - primary document (if provided)\n",
    "        - the accession 'complete submission text file' {accession}.txt (best-effort)\n",
    "        - all other files in index.json that match include_patterns and aren't excluded\n",
    "\n",
    "        Why include XBRL files:\n",
    "        - Even if you're doing RAG, keeping the raw XBRL instance files can be useful later.\n",
    "        - Some tables you care about (especially in 10-Q/10-K) may be represented in XBRL too.\n",
    "\n",
    "        Why exclude images/PDF by default:\n",
    "        - They're often heavy and less useful for text-based RAG.\n",
    "        - You can include them later if you decide to OCR or image-parse.\n",
    "        \"\"\"\n",
    "        ensure_dir(out_dir)\n",
    "        folder = self.filing_folder(cik, accession)\n",
    "\n",
    "        # 1) Save index.json (document inventory)\n",
    "        idx = self.filing_index_json(cik, accession)\n",
    "        (out_dir / \"index.json\").write_text(json.dumps(idx, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "        # 2) Download the primary doc (often the main HTML)\n",
    "        if primary_document:\n",
    "            self.download_file(f\"{folder}/{primary_document}\", out_dir / primary_document)\n",
    "\n",
    "        # 3) Best-effort download of \"complete submission text file\"\n",
    "        #    Not all accessions have it, but many do.\n",
    "        try:\n",
    "            self.download_file(f\"{folder}/{accession}.txt\", out_dir / f\"{accession}.txt\")\n",
    "        except requests.HTTPError:\n",
    "            pass\n",
    "\n",
    "        # 4) Download additional files from index.json\n",
    "        items = idx.get(\"directory\", {}).get(\"item\", [])\n",
    "        for it in items:\n",
    "            name = it.get(\"name\")\n",
    "            if not name:\n",
    "                continue\n",
    "\n",
    "            lname = name.lower()\n",
    "\n",
    "            # Exclude heavy/irrelevant types unless you explicitly want them\n",
    "            if exclude_patterns and any(lname.endswith(x) for x in exclude_patterns):\n",
    "                continue\n",
    "\n",
    "            # Only include relevant doc types\n",
    "            if include_patterns and not any(lname.endswith(x) for x in include_patterns):\n",
    "                continue\n",
    "\n",
    "            dest = out_dir / name\n",
    "            if dest.exists():\n",
    "                continue  # avoid re-downloading if already present\n",
    "\n",
    "            self.download_file(f\"{folder}/{name}\", dest)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03e32b",
   "metadata": {},
   "source": [
    "### **Convert filing HTML to “RAG artifacts” (text + tables)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375f6ed",
   "metadata": {},
   "source": [
    "Most LLM/RAG pipelines work best when:\n",
    "- narrative text is cleaned (scripts/styles removed, normalized whitespace)\n",
    "- tables are stored in a consistent format (CSV/JSON), and you keep them separately from narrative text\n",
    "\n",
    "We’ll parse the primary HTML into:\n",
    "- narrative_text.txt\n",
    "- tables/table_###.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52da8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_from_text(text: str) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Choose the correct parser automatically.\n",
    "\n",
    "    - XML → use XML parser\n",
    "    - HTML/XHTML → use HTML parser\n",
    "    \"\"\"\n",
    "    # Heuristic: XML docs almost always start with this\n",
    "    if text.lstrip().startswith(\"<?xml\"):\n",
    "        return BeautifulSoup(text, \"lxml-xml\")\n",
    "    else:\n",
    "        return BeautifulSoup(text, \"lxml\")\n",
    "\n",
    "\n",
    "def html_to_text_and_tables(html_path: Path) -> Tuple[str, List[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Convert a filing HTML into:\n",
    "    - narrative-ish text (for embeddings / summarization)\n",
    "    - a list of tables (as DataFrames)\n",
    "\n",
    "    Why BeautifulSoup:\n",
    "    - lets us remove scripts/styles and extract the visible text.\n",
    "\n",
    "    Why pandas.read_html:\n",
    "    - quick way to extract all <table> tags into DataFrames.\n",
    "    - filings can be messy; this won't be perfect, but it's a strong baseline.\n",
    "    \"\"\"\n",
    "    html = html_path.read_text(errors=\"ignore\")\n",
    "\n",
    "    # Parse the HTML document\n",
    "    soup = soup_from_text(html)\n",
    "\n",
    "    # Remove tags that add noise to extracted text\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Extract tables\n",
    "    # If there are no tables, read_html raises ValueError, so we handle that.\n",
    "    try:\n",
    "        tables = pd.read_html(html)\n",
    "    except ValueError:\n",
    "        tables = []\n",
    "\n",
    "    # Extract visible text with line breaks preserved\n",
    "    text = soup.get_text(\"\\n\")\n",
    "\n",
    "    # Normalize excessive line breaks\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
    "\n",
    "    return text, tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30abea",
   "metadata": {},
   "source": [
    "### **Driver function: download filings + prepare RAG folder structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a17cd4",
   "metadata": {},
   "source": [
    "**RAG downloader (complete filings)**\n",
    "\n",
    "Folder layout example:<br>\n",
    "sec_rag/<br>\n",
    "&ensp;CIK0000006201/<br>\n",
    "&ensp;&ensp;2025-10-23_10-Q_0000006201-25-000123/<br>\n",
    "&ensp;&ensp;&ensp;index.json<br>\n",
    "&ensp;&ensp;&ensp;metadata.json<br>\n",
    "&ensp;&ensp;&ensp;primarydoc.htm<br>\n",
    "&ensp;&ensp;&ensp;narrative_text.txt<br>\n",
    "&ensp;&ensp;tables/<br>\n",
    "&ensp;&ensp;&ensp;table_000.csv<br>\n",
    "&ensp;&ensp;&ensp;table_001.csv<br>\n",
    "&ensp;&ensp;0000006201-25-000123.txt<br>\n",
    "&ensp;&ensp;...<br>\n",
    "\n",
    "Why this structure:\n",
    "- easy to re-run incrementally (new accessions get new folders)\n",
    "- easy to index in a vector DB (metadata + text paths)\n",
    "- easy to audit / reproduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d246a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_filings_for_rag(\n",
    "    client: SecClient,\n",
    "    cik: str | int,\n",
    "    out_root: Path,\n",
    "    forms: Iterable[str] = (\"8-K\", \"10-Q\", \"10-K\", \"ARS\"),\n",
    "    limit: int = 500,\n",
    "    *,\n",
    "    windows: List[Tuple[str, datetime, datetime]],\n",
    "    date_field: Literal[\"filingDate\", \"reportDate\"] = \"reportDate\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Download a set of filings for a company and produce RAG-friendly artifacts.\n",
    "\n",
    "    Steps:\n",
    "    1) List recent filings via Submissions JSON.\n",
    "    2) For each filing accession:\n",
    "       - download the accession package (primary HTML, complete submission text, and other docs)\n",
    "       - extract narrative text + tables from the primary HTML (if present)\n",
    "       - save metadata.json for later indexing\n",
    "\n",
    "    Why 'limit':\n",
    "    - keeps dev/testing fast\n",
    "    - you can expand later once you're confident in your pipeline\n",
    "    \"\"\"\n",
    "    cik10 = normalize_cik(cik)\n",
    "    company_dir = out_root / f\"CIK{cik10}\"\n",
    "    ensure_dir(company_dir)\n",
    "\n",
    "    # 1) Get recent filings of the desired forms\n",
    "    filings = client.list_recent_filings(\n",
    "        cik,\n",
    "        forms=forms,\n",
    "        limit=limit,\n",
    "        windows=windows,\n",
    "        date_field=date_field,\n",
    "    )\n",
    "\n",
    "    for f in filings:\n",
    "        # 2) Core identifiers\n",
    "        accn = f[\"accessionNumber\"]\n",
    "        form = f.get(\"form\")\n",
    "        filed = f.get(\"filingDate\")\n",
    "        report_date = f.get(\"reportDate\")\n",
    "        primary = f.get(\"primaryDocument\")\n",
    "        matched = f.get(\"matched_windows\", [])\n",
    "\n",
    "        # 3) Create a deterministic folder per accession and skip if already done\n",
    "        acc_dir = company_dir / f\"{filed}_{form}_{accn}\"\n",
    "        if (acc_dir / \"metadata.json\").exists():\n",
    "            continue\n",
    "        ensure_dir(acc_dir)\n",
    "\n",
    "        # 4) Download the full accession package\n",
    "        client.download_accession_package(\n",
    "            cik=cik,\n",
    "            accession=accn,\n",
    "            primary_document=primary,\n",
    "            out_dir=acc_dir,\n",
    "            include_patterns=(\".htm\", \".html\", \".txt\", \".xml\", \".xsd\", \".cal\", \".lab\", \".pre\", \".json\"),\n",
    "            exclude_patterns=(\".jpg\", \".png\", \".gif\", \".svg\"),\n",
    "        )\n",
    "\n",
    "        # 5) Convert primary HTML to narrative text + table CSVs\n",
    "        if primary:\n",
    "            primary_path = acc_dir / primary\n",
    "            if primary_path.exists() and primary_path.suffix.lower() in (\".htm\", \".html\"):\n",
    "                text, tables = html_to_text_and_tables(primary_path)\n",
    "\n",
    "                # Save narrative text for embeddings / LLM summarization\n",
    "                (acc_dir / \"narrative_text.txt\").write_text(text, encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "                # Save extracted tables as CSVs\n",
    "                tdir = acc_dir / \"tables\"\n",
    "                ensure_dir(tdir)\n",
    "\n",
    "                # Cap number of tables to avoid huge output for very table-heavy documents\n",
    "                for i, df in enumerate(tables[:300]):\n",
    "                    df.to_csv(tdir / f\"table_{i:03d}.csv\", index=False)\n",
    "\n",
    "        # 6) Save metadata for vector DB indexing and provenance\n",
    "        meta = {\n",
    "            \"cik\": cik10,\n",
    "            \"accession\": accn,\n",
    "            \"form\": form,\n",
    "            \"filed\": filed,\n",
    "            \"reportDate\": report_date,\n",
    "            \"primaryDocument\": primary,\n",
    "            \"date_field_used_for_filtering\": date_field,\n",
    "            \"matched_windows\": matched,  # <- key addition\n",
    "        }\n",
    "        (acc_dir / \"metadata.json\").write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913067d3",
   "metadata": {},
   "source": [
    "### **RAG Materials Download**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a66d698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trica\\AppData\\Local\\Temp\\ipykernel_16448\\3310521992.py:40: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(html)\n",
      "c:\\Users\\trica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\html.py:661: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(udoc, features=\"html5lib\", from_encoding=from_encoding)\n",
      "C:\\Users\\trica\\AppData\\Local\\Temp\\ipykernel_16448\\3310521992.py:40: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(html)\n",
      "c:\\Users\\trica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\html.py:661: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(udoc, features=\"html5lib\", from_encoding=from_encoding)\n",
      "C:\\Users\\trica\\AppData\\Local\\Temp\\ipykernel_16448\\3310521992.py:40: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(html)\n",
      "c:\\Users\\trica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\html.py:661: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(udoc, features=\"html5lib\", from_encoding=from_encoding)\n",
      "C:\\Users\\trica\\AppData\\Local\\Temp\\ipykernel_16448\\3310521992.py:40: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(html)\n",
      "c:\\Users\\trica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\html.py:661: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(udoc, features=\"html5lib\", from_encoding=from_encoding)\n"
     ]
    }
   ],
   "source": [
    "# Set up the SEC client using the previously defined user agent\n",
    "client = SecClient(user_agent=USER_AGENT)\n",
    "\n",
    "# Set your output root directory\n",
    "out_root = Path(\"SEC_Filings/EDGAR_raw\")\n",
    "\n",
    "# Define the airlines(s) of interest and generate their CIK mapping\n",
    "airlines = [\"AAL\", \"DAL\", \"UAL\", \"LUV\"]\n",
    "airline_ciks = build_airline_cik_dict(airlines, ticker_map)\n",
    "\n",
    "# Define the period(s) of interest and generate filter windows\n",
    "years = [2025]\n",
    "periods = [\"FY\"]\n",
    "windows = build_date_windows(years, periods)\n",
    "\n",
    "# Define date field to filter on (functions default to \"reportDate\" but this makes it explicit)\n",
    "date_field = \"reportDate\"\n",
    "\n",
    "# Run for all airlines present in data\n",
    "for ticker, cik in airline_ciks.items():\n",
    "    download_filings_for_rag(\n",
    "        client,\n",
    "        cik=cik,\n",
    "        out_root=out_root,\n",
    "        windows=windows,\n",
    "        date_field=date_field,\n",
    "        limit=1000,  # adjust as necessary\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
