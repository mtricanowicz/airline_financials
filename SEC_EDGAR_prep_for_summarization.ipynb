{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528677c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def read_text_safely(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Read text with UTF-8, replacing characters that Windows cp1252 can't encode.\n",
    "    This prevents UnicodeEncodeError / decoding problems later.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        return \"\"\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "\n",
    "def read_json(path: Path) -> Dict[str, Any]:\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\", errors=\"replace\"))\n",
    "\n",
    "\n",
    "def iso_or_none(dt: Any) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Normalize a datetime-ish value to ISO string, else None.\n",
    "    Accepts strings already in ISO-ish format.\n",
    "    \"\"\"\n",
    "    if dt is None:\n",
    "        return None\n",
    "    if isinstance(dt, str):\n",
    "        return dt\n",
    "    if isinstance(dt, datetime):\n",
    "        return dt.isoformat()\n",
    "    return str(dt)\n",
    "\n",
    "\n",
    "def sanitize_filename(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    *,\n",
    "    chunk_chars: int = 3500,\n",
    "    overlap_chars: int = 250,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunk long narrative text into page-like chunks (stable “page_label/total_pages”).\n",
    "\n",
    "    Why char-based chunking:\n",
    "    - Simple and robust (works for any filing text)\n",
    "    - Keeps each chunk to a manageable size for embeddings/LLM\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "    while i < n:\n",
    "        j = min(i + chunk_chars, n)\n",
    "        chunks.append(text[i:j])\n",
    "        if j == n:\n",
    "            break\n",
    "        i = max(0, j - overlap_chars)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def load_tables_as_records(tables_dir: Path, max_tables: int = 300) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load CSV tables as structured records.\n",
    "    You can skip this entirely if you only want narrative text.\n",
    "    \"\"\"\n",
    "    if not tables_dir.exists():\n",
    "        return []\n",
    "\n",
    "    table_files = sorted(tables_dir.glob(\"table_*.csv\"))[:max_tables]\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for tf in table_files:\n",
    "        try:\n",
    "            df = pd.read_csv(tf)\n",
    "            out.append({\n",
    "                \"file\": tf.name,\n",
    "                \"n_rows\": int(df.shape[0]),\n",
    "                \"n_cols\": int(df.shape[1]),\n",
    "                \"columns\": df.columns.tolist(),\n",
    "                # If you want full table data embedded, uncomment:\n",
    "                # \"data\": df.fillna(\"\").astype(str).values.tolist()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            out.append({\"file\": tf.name, \"error\": str(e)})\n",
    "    return out\n",
    "\n",
    "\n",
    "def collection_filename(ticker: str, period: PeriodSpec) -> str:\n",
    "    \"\"\"\n",
    "    Returns: TTTYYYYPP_filings.json\n",
    "    Example: AAL2022Q3_filings.json\n",
    "    \"\"\"\n",
    "    return f\"{ticker.upper()}{period.key}_filings.json\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Period Key Class\n",
    "# -----------------------------\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PeriodSpec:\n",
    "    year: int      # e.g. 2022\n",
    "    period: str    # \"Q1\",\"Q2\",\"Q3\",\"Q4\",\"FY\"\n",
    "\n",
    "    @property\n",
    "    def key(self) -> str:\n",
    "        \"\"\"YYYYPP → e.g. 2022Q3, 2023FY\"\"\"\n",
    "        return f\"{self.year}{self.period.upper()}\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_label(cls, label: str) -> \"PeriodSpec\":\n",
    "        \"\"\"\n",
    "        Accepts labels like:\n",
    "          - '2022-Q3'\n",
    "          - '2022Q3'\n",
    "          - '2023-FY'\n",
    "        \"\"\"\n",
    "        m = re.match(r\"(\\d{4})[-]?(Q[1-4]|FY)\", label.upper())\n",
    "        if not m:\n",
    "            raise ValueError(f\"Invalid period label: {label}\")\n",
    "        return cls(year=int(m.group(1)), period=m.group(2))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Core: build a single “collection JSON” for one airline and one period label\n",
    "# -----------------------------\n",
    "\n",
    "def accession_dirs_for_cik(out_root: Path, cik10: str) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Return all accession directories under sec_rag/CIK##########/\n",
    "    \"\"\"\n",
    "    company_dir = out_root / f\"CIK{cik10}\"\n",
    "    if not company_dir.exists():\n",
    "        return []\n",
    "    return [p for p in company_dir.iterdir() if p.is_dir()]\n",
    "\n",
    "\n",
    "def build_collection_json_for_airline_period(\n",
    "    *,\n",
    "    out_root: Path,\n",
    "    cik10: str,\n",
    "    ticker: str,\n",
    "    period: PeriodSpec,\n",
    "    out_file: Path,\n",
    "    include_tables: bool = True,\n",
    "    chunk_chars: int = 3500,\n",
    "    overlap_chars: int = 250,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Creates one JSON file like your attached example: a list of records.\n",
    "\n",
    "    Each record corresponds to a chunk (“page”) of narrative text from a filing,\n",
    "    with metadata fields carried along for provenance and retrieval.\n",
    "    \"\"\"\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for acc_dir in accession_dirs_for_cik(out_root, cik10):\n",
    "        meta_path = acc_dir / \"metadata.json\"\n",
    "        meta = read_json(meta_path)\n",
    "        if not meta:\n",
    "            continue\n",
    "\n",
    "        # If you used matched_windows in metadata.json, filter by the period label\n",
    "        matched = meta.get(\"matched_windows\") or []\n",
    "        period_dash = f\"{period.year}-{period.period.upper()}\"   # \"2025-Q3\"\n",
    "        period_nodash = period.key                               # \"2025Q3\"\n",
    "        if (period_dash not in matched) and (period_nodash not in matched):\n",
    "            continue\n",
    "\n",
    "        form = meta.get(\"form\") or \"UNKNOWN\"\n",
    "        filed = meta.get(\"filed\") or meta.get(\"filingDate\") or meta.get(\"filing_date\")\n",
    "        report_date = meta.get(\"reportDate\")\n",
    "\n",
    "        # Optional: the SEC archives folder URL if you stored it; else leave blank\n",
    "        source = meta.get(\"source\") or meta.get(\"archives_url\") or \"\"\n",
    "\n",
    "        # Title (matches your example’s “Form 10-K for ... filed ...” style)\n",
    "        title = meta.get(\"title\")\n",
    "        if not title:\n",
    "            title = f\"Form {form} for {ticker} (accession {meta.get('accession','')})\"\n",
    "\n",
    "        # Narrative text chunks\n",
    "        narrative_path = acc_dir / \"narrative_text.txt\"\n",
    "        narrative = read_text_safely(narrative_path)\n",
    "        chunks = chunk_text(narrative, chunk_chars=chunk_chars, overlap_chars=overlap_chars)\n",
    "\n",
    "        total_pages = len(chunks) if chunks else 1\n",
    "\n",
    "        if chunks:\n",
    "            for idx, chunk in enumerate(chunks, start=1):\n",
    "                rec_id = f\"{ticker}{period.key}-{len(records)}-{title}-Page {idx} of {total_pages}\"\n",
    "                records.append({\n",
    "                    \"id\": rec_id,\n",
    "                    \"airline\": ticker,\n",
    "                    \"title\": title,\n",
    "                    \"form\": form,\n",
    "                    \"date_filed\": iso_or_none(filed),\n",
    "                    \"report_date\": iso_or_none(report_date),\n",
    "                    \"page_label\": str(idx),\n",
    "                    \"total_pages\": total_pages,\n",
    "                    \"source\": source,\n",
    "                    \"text\": chunk,\n",
    "                })\n",
    "        else:\n",
    "            # If no narrative_text.txt exists, still create one record with empty text\n",
    "            rec_id = f\"{ticker}{period.key}-{len(records)}-{title}-Page 1 of 1\"\n",
    "            records.append({\n",
    "                \"id\": rec_id,\n",
    "                \"airline\": ticker,\n",
    "                \"title\": title,\n",
    "                \"form\": form,\n",
    "                \"date_filed\": iso_or_none(filed),\n",
    "                \"report_date\": iso_or_none(report_date),\n",
    "                \"page_label\": \"1\",\n",
    "                \"total_pages\": 1,\n",
    "                \"source\": source,\n",
    "                \"text\": \"\",\n",
    "            })\n",
    "\n",
    "        # Optional: add table inventory as an extra “record” per filing\n",
    "        if include_tables:\n",
    "            tables_dir = acc_dir / \"tables\"\n",
    "            table_info = load_tables_as_records(tables_dir)\n",
    "            if table_info:\n",
    "                rec_id = f\"{ticker}{period.key}-{len(records)}-{title}-Tables\"\n",
    "                records.append({\n",
    "                    \"id\": rec_id,\n",
    "                    \"airline\": ticker,\n",
    "                    \"title\": f\"{title} (Tables)\",\n",
    "                    \"form\": form,\n",
    "                    \"date_filed\": iso_or_none(filed),\n",
    "                    \"report_date\": iso_or_none(report_date),\n",
    "                    \"page_label\": \"tables\",\n",
    "                    \"total_pages\": total_pages,\n",
    "                    \"source\": source,\n",
    "                    \"tables\": table_info,\n",
    "                })\n",
    "\n",
    "    out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_file.write_text(json.dumps(records, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return out_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f15ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_ciks = {\n",
    "    #\"AAL\": \"0000006201\",\n",
    "    \"DAL\": \"0000027904\"#,\n",
    "    #\"UAL\": \"0000100517\",\n",
    "    #\"LUV\": \"0000092380\",\n",
    "}\n",
    "\n",
    "periods_initial = [\n",
    "    PeriodSpec(2014, \"Q1\"),\n",
    "    PeriodSpec(2014, \"Q2\"),\n",
    "    PeriodSpec(2014, \"Q3\"),\n",
    "    PeriodSpec(2014, \"Q4\"),\n",
    "    PeriodSpec(2014, \"FY\"),\n",
    "    PeriodSpec(2015, \"Q1\"),\n",
    "    PeriodSpec(2015, \"Q2\"),\n",
    "    PeriodSpec(2015, \"Q3\"),\n",
    "    PeriodSpec(2015, \"Q4\"),\n",
    "    PeriodSpec(2015, \"FY\"),\n",
    "    PeriodSpec(2016, \"Q1\"),\n",
    "    PeriodSpec(2016, \"Q2\"),\n",
    "    PeriodSpec(2016, \"Q3\"),\n",
    "    PeriodSpec(2016, \"Q4\"),\n",
    "    PeriodSpec(2016, \"FY\"),\n",
    "    PeriodSpec(2017, \"Q1\"),\n",
    "    PeriodSpec(2017, \"Q2\"),\n",
    "    PeriodSpec(2017, \"Q3\"),\n",
    "    PeriodSpec(2017, \"Q4\"),\n",
    "    PeriodSpec(2017, \"FY\"),\n",
    "    PeriodSpec(2018, \"Q1\"),\n",
    "    PeriodSpec(2018, \"Q2\"),\n",
    "    PeriodSpec(2018, \"Q3\"),\n",
    "    PeriodSpec(2018, \"Q4\"),\n",
    "    PeriodSpec(2018, \"FY\"),\n",
    "    PeriodSpec(2019, \"Q1\"),\n",
    "    PeriodSpec(2019, \"Q2\"),\n",
    "    PeriodSpec(2019, \"Q3\"),\n",
    "    PeriodSpec(2019, \"Q4\"),\n",
    "    PeriodSpec(2019, \"FY\"),\n",
    "    PeriodSpec(2020, \"Q1\"),\n",
    "    PeriodSpec(2020, \"Q2\"),\n",
    "    PeriodSpec(2020, \"Q3\"),\n",
    "    PeriodSpec(2020, \"Q4\"),\n",
    "    PeriodSpec(2020, \"FY\"),\n",
    "    PeriodSpec(2021, \"Q1\"),\n",
    "    PeriodSpec(2021, \"Q2\"),\n",
    "    PeriodSpec(2021, \"Q3\"),\n",
    "    PeriodSpec(2021, \"Q4\"),\n",
    "    PeriodSpec(2021, \"FY\"),\n",
    "    PeriodSpec(2022, \"Q1\"),\n",
    "    PeriodSpec(2022, \"Q2\"),\n",
    "    PeriodSpec(2022, \"Q3\"),\n",
    "    PeriodSpec(2022, \"Q4\"),\n",
    "    PeriodSpec(2022, \"FY\"),\n",
    "    PeriodSpec(2023, \"Q1\"),\n",
    "    PeriodSpec(2023, \"Q2\"),\n",
    "    PeriodSpec(2023, \"Q3\"),\n",
    "    PeriodSpec(2023, \"Q4\"),\n",
    "    PeriodSpec(2023, \"FY\"),\n",
    "    PeriodSpec(2024, \"Q1\"),\n",
    "    PeriodSpec(2024, \"Q2\"),\n",
    "    PeriodSpec(2024, \"Q3\"),\n",
    "    PeriodSpec(2024, \"Q4\"),\n",
    "    PeriodSpec(2024, \"FY\"),\n",
    "    PeriodSpec(2025, \"Q1\"),\n",
    "    PeriodSpec(2025, \"Q2\"),\n",
    "    PeriodSpec(2025, \"Q3\"),\n",
    "    PeriodSpec(2025, \"Q4\"),\n",
    "    PeriodSpec(2025, \"FY\"),\n",
    "]\n",
    "\n",
    "periods_current = [\n",
    "    PeriodSpec(2026, \"Q1\")\n",
    "]\n",
    "\n",
    "periods = periods_current\n",
    "\n",
    "for ticker, cik10 in airline_ciks.items():\n",
    "    for p in periods:\n",
    "        out_file = Path(\"SEC_Filings/EDGAR\") / collection_filename(ticker, p)\n",
    "\n",
    "        build_collection_json_for_airline_period(\n",
    "            out_root=Path(\"SEC_Filings/EDGAR_raw\"),\n",
    "            cik10=cik10,\n",
    "            ticker=ticker,\n",
    "            period=PeriodSpec.from_label(f\"{p.year}-{p.period}\"),\n",
    "            out_file=out_file,\n",
    "            include_tables=True,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
